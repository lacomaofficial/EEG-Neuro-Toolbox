{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16bcb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ultra-fast group connectivity analysis with PHASE SEGMENTS...\n",
      "📥 Loading ROI names...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_atlas_difumo</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset found in <span style=\"color: #800080; text-decoration-color: #800080\">/home/jaizor/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">difumo_atlases</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mfetch_atlas_difumo\u001b[0m\u001b[1;34m]\u001b[0m Dataset found in \u001b[35m/home/jaizor/nilearn_data/\u001b[0m\u001b[95mdifumo_atlases\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Found 11 subjects\n",
      "🚀 Processing 11 subjects in parallel using 39 cores...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loaded (512, 131679) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 248466) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263506) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263524) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 215473) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263481) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263515) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263356) from difumo_time_courses.npy📥 Loaded (512, 210656) from difumo_time_courses.npy\n",
      "\n",
      "📥 Loaded (512, 263523) from difumo_time_courses.npy\n",
      "📥 Loaded (512, 263517) from difumo_time_courses.npy\n",
      "⚠️  No epochs for InPhase in sub-11\n",
      "⚠️  No epochs for OutofPhase in sub-11\n",
      "✅ sub-11: 0 matrices (1/11)\n",
      "⚠️  No epochs for InPhase in sub-08\n",
      "⚠️  No epochs for OutofPhase in sub-08\n",
      "⚠️  No epochs for InPhase in sub-14\n",
      "⚠️  No epochs for OutofPhase in sub-14\n",
      "⚠️  No epochs for InPhase in sub-07\n",
      "⚠️  No epochs for OutofPhase in sub-07\n",
      "⚠️  No epochs for InPhase in sub-06\n",
      "⚠️  No epochs for OutofPhase in sub-06\n",
      "⚠️  No epochs for InPhase in sub-01\n",
      "⚠️  No epochs for OutofPhase in sub-01\n",
      "⚠️  No epochs for InPhase in sub-02\n",
      "⚠️  No epochs for InPhase in sub-12\n",
      "⚠️  No epochs for OutofPhase in sub-12\n",
      "⚠️  No epochs for InPhase in sub-09\n",
      "⚠️  No epochs for OutofPhase in sub-09\n",
      "✅ sub-08: 0 matrices (2/11)\n",
      "✅ sub-14: 0 matrices (3/11)\n",
      "✅ sub-07: 0 matrices (4/11)\n",
      "✅ sub-06: 0 matrices (5/11)\n",
      "✅ sub-01: 0 matrices (6/11)\n",
      "✅ sub-12: 0 matrices (7/11)\n",
      "✅ sub-09: 0 matrices (8/11)\n",
      "✅ sub-02: 6 matrices (9/11)\n",
      "✅ sub-10: 12 matrices (10/11)\n",
      "✅ sub-05: 12 matrices (11/11)\n",
      "\n",
      "⚡ Computing group averages...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 449\u001b[39m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   • Speed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_subjects/elapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m matrices/second\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 425\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# Phase 2: Compute averages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m group_averages = compute_fast_averages(all_matrices)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_averages:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ No group averages computed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 368\u001b[39m, in \u001b[36mcompute_fast_averages\u001b[39m\u001b[34m(all_matrices)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# Stack and average in one optimized operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m stacked = np.stack(matrix_list, axis=\u001b[32m0\u001b[39m, dtype=np.float32)\n\u001b[32m    369\u001b[39m group_avg = np.mean(stacked, axis=\u001b[32m0\u001b[39m, dtype=np.float32)\n\u001b[32m    371\u001b[39m group_averages[(condition, band_name)] = group_avg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jaizor/xtra/miniconda3/envs/Xtra/lib/python3.13/site-packages/numpy/_core/shape_base.py:460\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    458\u001b[39m shapes = {arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mall input arrays must have the same shape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    462\u001b[39m result_ndim = arrays[\u001b[32m0\u001b[39m].ndim + \u001b[32m1\u001b[39m\n\u001b[32m    463\u001b[39m axis = normalize_axis_index(axis, result_ndim)\n",
      "\u001b[31mValueError\u001b[39m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "# compute_group_connectivity_to_csv_fast.py\n",
    "# Ultra-fast version with parallel processing and memory optimizations\n",
    "# Key improvements:\n",
    "# - Parallel subject processing\n",
    "# - Batch connectivity computation\n",
    "# - Memory safety (no mmap corruption)\n",
    "# - Optimized MNE operations\n",
    "# - NOW INTEGRATED WITH PHASE-SEGMENT EPOCHS (10s In/Out of Phase)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from nilearn import datasets\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "# Configuration\n",
    "PROJECT_BASE = '/home/jaizor/jaizor/xtra'\n",
    "BASE_DIR = Path(PROJECT_BASE)\n",
    "GROUP_OUTPUT_DIR = Path(PROJECT_BASE) / \"derivatives\" / \"group2\"/ '10s'\n",
    "GROUP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BANDS = {\n",
    "    \"Theta\": (4, 8),\n",
    "    \"Alpha\": (8, 12),\n",
    "    \"Low_Beta\": (13, 20),\n",
    "    \"High_Beta\": (20, 30),\n",
    "    \"Low_Gamma\": (30, 60),\n",
    "    \"High_Gamma\": (60, 100)\n",
    "}\n",
    "\n",
    "METHOD = 'wpli2_debiased'\n",
    "SFREQ = 500.0\n",
    "N_ROIS = 512\n",
    "CONDITIONS = ['InPhase', 'OutofPhase']\n",
    "\n",
    "# Use all available cores minus 1\n",
    "N_JOBS = max(1, mp.cpu_count() - 1)\n",
    "\n",
    "\n",
    "def load_roi_names() -> List[str]:\n",
    "    \"\"\"Load and clean DiFuMo ROI names once.\"\"\"\n",
    "    try:\n",
    "        atlas = datasets.fetch_atlas_difumo(dimension=512, resolution_mm=2)\n",
    "        roi_names = atlas.labels['difumo_names'].astype(str).tolist()\n",
    "    except Exception:\n",
    "        roi_names = [f\"Component_{i}\" for i in range(N_ROIS)]\n",
    "    \n",
    "    # Clean names for CSV compatibility\n",
    "    return [name.replace(',', ';').replace('\\n', ' ').replace('\\r', ' ') \n",
    "            for name in roi_names]\n",
    "\n",
    "\n",
    "def find_subjects() -> List[str]:\n",
    "    \"\"\"Find all subjects with complete data including phase epochs.\"\"\"\n",
    "    eeg_dir = BASE_DIR / \"derivatives\" / \"eeg\"\n",
    "    if not eeg_dir.exists():\n",
    "        return []\n",
    "\n",
    "    subjects = []\n",
    "    for item in eeg_dir.iterdir():\n",
    "        if not (item.is_dir() and item.name.startswith(\"sub-\")):\n",
    "            continue\n",
    "            \n",
    "        # Check required files exist\n",
    "        events_dir = item / \"bima_DBSOFF\"\n",
    "        lcmv_dir = BASE_DIR / \"derivatives\" / \"lcmv\" / f\"{item.name}_bima_full_off\"\n",
    "        epochs_dir = BASE_DIR / \"derivatives\" / \"epochs\" / item.name\n",
    "\n",
    "        required_files = [\n",
    "            lcmv_dir / \"difumo_time_courses.npy\",\n",
    "            events_dir / f\"{item.name}_events_mne_binary-eve.fif\",\n",
    "            events_dir / f\"{item.name}_event_id_binary.json\",\n",
    "            epochs_dir / f\"{item.name}_in_phase-epo.fif\",\n",
    "            epochs_dir / f\"{item.name}_out_of_phase-epo.fif\",\n",
    "        ]\n",
    "        \n",
    "        if all(f.exists() for f in required_files):\n",
    "            subjects.append(item.name)\n",
    "\n",
    "    return sorted(subjects, key=lambda x: int(x.split('-')[1]))\n",
    "\n",
    "\n",
    "def load_phase_epochs(subject: str, project_base: str = PROJECT_BASE) -> Optional[Dict[str, mne.Epochs]]:\n",
    "    \"\"\"\n",
    "    Load precomputed In-Phase and Out-of-Phase epochs from disk.\n",
    "    Returns dict: {'InPhase': epochs_in, 'OutofPhase': epochs_out}\n",
    "    \"\"\"\n",
    "    base_path = Path(project_base)\n",
    "    epochs_dir = base_path / \"derivatives\" / \"epochs\" / subject\n",
    "\n",
    "    in_file = epochs_dir / f\"{subject}_in_phase-epo.fif\"\n",
    "    out_file = epochs_dir / f\"{subject}_out_of_phase-epo.fif\"\n",
    "\n",
    "    if not in_file.exists() or not out_file.exists():\n",
    "        print(f\"❌ Missing epoch files for {subject}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        epochs_in = mne.read_epochs(in_file, preload=True, verbose=False)\n",
    "        epochs_out = mne.read_epochs(out_file, preload=True, verbose=False)\n",
    "        return {'InPhase': epochs_in, 'OutofPhase': epochs_out}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load epochs for {subject}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_epochs_from_phase_segments(subject, project_base=PROJECT_BASE, tmin_in=-10.0, tmax_in=0.0, tmin_out=0.0, tmax_out=10.0):\n",
    "    \"\"\"\n",
    "    Create MNE Epochs for In-Phase and Out-of-Phase segments based on detected phase breaks.\n",
    "    Saves to disk and returns dict.\n",
    "    \"\"\"\n",
    "    project_base = Path(project_base)\n",
    "    \n",
    "    # --- Load phase segments ---\n",
    "    segments_file = project_base / \"derivatives\" / \"phase_segments\" / f\"{subject}_phase_segments.csv\"\n",
    "    if not segments_file.exists():\n",
    "        print(f\"❌ Phase segments not found for {subject}\")\n",
    "        return None\n",
    "    \n",
    "    segments_df = pd.read_csv(segments_file)\n",
    "    if segments_df.empty:\n",
    "        print(f\"❌ No phase segments for {subject}\")\n",
    "        return None\n",
    "\n",
    "    # --- Load RAW EEG ---\n",
    "    raw_file = project_base / \"derivatives\" / \"eeg\" / subject / \"bima_DBSOFF\" / f\"{subject}_ses-DBSOFF_task-bima_eeg_ica_cleaned_raw.fif\"\n",
    "    if not raw_file.exists():\n",
    "        print(f\"❌ RAW file not found for {subject}\")\n",
    "        return None\n",
    "\n",
    "    raw = mne.io.read_raw_fif(str(raw_file), preload=True)\n",
    "    sfreq = raw.info['sfreq']\n",
    "\n",
    "    # --- Create events array from break_time ---\n",
    "    break_times = segments_df['break_time'].values\n",
    "    event_samples = (break_times * sfreq).astype(int)\n",
    "    event_id_in = 1  # arbitrary, for in-phase\n",
    "\n",
    "    events = np.column_stack([\n",
    "        event_samples,\n",
    "        np.zeros(len(event_samples), dtype=int),\n",
    "        np.ones(len(event_samples), dtype=int) * event_id_in\n",
    "    ])\n",
    "\n",
    "    # --- Create metadata ---\n",
    "    metadata = segments_df.copy()\n",
    "    metadata['event_time'] = break_times\n",
    "    metadata['event_sample'] = event_samples\n",
    "\n",
    "    # --- Create Epochs: In-Phase (10s before break) ---\n",
    "    print(f\"🧠 Creating In-Phase epochs for {subject} (tmin={tmin_in}, tmax={tmax_in})...\")\n",
    "    epochs_in = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id={'phase_break': event_id_in},\n",
    "        tmin=tmin_in,\n",
    "        tmax=tmax_in,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # --- Create Epochs: Out-of-Phase (10s after break) ---\n",
    "    print(f\"🔥 Creating Out-of-Phase epochs for {subject} (tmin={tmin_out}, tmax={tmax_out})...\")\n",
    "    epochs_out = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id={'phase_break': event_id_in},\n",
    "        tmin=tmin_out,\n",
    "        tmax=tmax_out,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Created {len(epochs_in)} In-Phase and {len(epochs_out)} Out-of-Phase epochs for {subject}\")\n",
    "\n",
    "    # --- Save to disk ---\n",
    "    output_dir = Path(project_base) / \"derivatives\" / \"epochs\" / subject\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    epochs_in.save(output_dir / f\"{subject}_in_phase-epo.fif\", overwrite=True)\n",
    "    epochs_out.save(output_dir / f\"{subject}_out_of_phase-epo.fif\", overwrite=True)\n",
    "    \n",
    "    print(f\"💾 Saved epochs to {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        'in_phase_epochs': epochs_in,\n",
    "        'out_of_phase_epochs': epochs_out\n",
    "    }\n",
    "\n",
    "\n",
    "def ensure_epochs_exist(subjects: List[str]):\n",
    "    \"\"\"Ensure phase epochs exist for all subjects — create if missing.\"\"\"\n",
    "    for subject in subjects:\n",
    "        epochs_dir = Path(PROJECT_BASE) / \"derivatives\" / \"epochs\" / subject\n",
    "        in_file = epochs_dir / f\"{subject}_in_phase-epo.fif\"\n",
    "        out_file = epochs_dir / f\"{subject}_out_of_phase-epo.fif\"\n",
    "\n",
    "        if not in_file.exists() or not out_file.exists():\n",
    "            print(f\"⏳ Creating epochs for {subject}...\")\n",
    "            epochs_dict = create_epochs_from_phase_segments(subject)\n",
    "            if epochs_dict:\n",
    "                print(f\"✅ Saved epochs for {subject}\")\n",
    "            else:\n",
    "                print(f\"❌ Failed to create epochs for {subject}\")\n",
    "\n",
    "\n",
    "def compute_single_connectivity(epoch_data: np.ndarray, band_range: Tuple[float, float]) -> Optional[np.ndarray]:\n",
    "    \"\"\"Compute connectivity matrix for given epochs and frequency band - optimized version.\"\"\"\n",
    "    if len(epoch_data) == 0:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Use fewer tapers for speed while maintaining quality\n",
    "        con = spectral_connectivity_epochs(\n",
    "            data=epoch_data,\n",
    "            method=METHOD,\n",
    "            mode='multitaper',\n",
    "            sfreq=SFREQ,\n",
    "            fmin=band_range[0],\n",
    "            fmax=band_range[1],\n",
    "            faverage=True,\n",
    "            verbose=False,\n",
    "            n_jobs=1,  # Each process handles one job\n",
    "            mt_bandwidth=2,  # Reduced bandwidth for speed\n",
    "            mt_low_bias=True\n",
    "        )\n",
    "        \n",
    "        matrix = con.get_data(output='dense').squeeze()\n",
    "        \n",
    "        # Fast symmetrization and diagonal zeroing\n",
    "        matrix = np.maximum(matrix, matrix.T)\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        \n",
    "        return matrix.astype(np.float32)  # Use float32 to save memory\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Connectivity computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_subject_parallel(subject: str) -> Dict[Tuple[str, str], np.ndarray]:\n",
    "    \"\"\"Process one subject - designed for parallel execution.\"\"\"\n",
    "    # File paths\n",
    "    data_file = BASE_DIR / \"derivatives\" / \"lcmv\" / f\"{subject}_bima_full_off\" / \"difumo_time_courses.npy\"\n",
    "    \n",
    "    subject_matrices = {}\n",
    "    \n",
    "    try:\n",
    "        # 🛡️ SAFE LOAD: Avoid mmap to prevent file corruption\n",
    "        data = np.load(data_file)  # Load fully into memory\n",
    "        print(f\"📥 Loaded {data.shape} from {data_file.name}\")\n",
    "\n",
    "        if data.size == 0:\n",
    "            print(f\"⚠️  Empty data in {data_file}\")\n",
    "            return {}\n",
    "\n",
    "        # Ensure (channels, time) format\n",
    "        if data.shape[0] > data.shape[1]:\n",
    "            data = data.T.copy()  # 🚨 COPY to ensure data ownership — critical fix!\n",
    "        else:\n",
    "            data = data.copy()  # Still copy to be safe\n",
    "\n",
    "        # Create MNE info object\n",
    "        info = mne.create_info(\n",
    "            ch_names=[f'C{i}' for i in range(N_ROIS)], \n",
    "            sfreq=SFREQ, \n",
    "            ch_types='misc'\n",
    "        )\n",
    "        raw = mne.io.RawArray(data, info, verbose=False)\n",
    "        \n",
    "        # 🆕 Load precomputed phase-based epochs\n",
    "        phase_epochs = load_phase_epochs(subject, PROJECT_BASE)\n",
    "        if not phase_epochs:\n",
    "            print(f\"⚠️  Skipping {subject} — no phase epochs found\")\n",
    "            return {}\n",
    "\n",
    "        # Process both conditions: InPhase and OutofPhase\n",
    "        for condition in CONDITIONS:\n",
    "            epochs = phase_epochs.get(condition)\n",
    "            if epochs is None:\n",
    "                print(f\"⚠️  No epochs for {condition} in {subject}\")\n",
    "                continue\n",
    "\n",
    "            # ⚡ Optional: Downsample to 250Hz to speed up connectivity (safe for bands < 120Hz)\n",
    "            # epochs = epochs.copy().resample(250.0, npad='auto')\n",
    "\n",
    "            epoch_data = epochs.get_data()\n",
    "            if len(epoch_data) == 0:\n",
    "                print(f\"⚠️  No epochs for {condition} in {subject}\")\n",
    "                continue\n",
    "\n",
    "            # Compute connectivity for all bands\n",
    "            for band_name, band_range in BANDS.items():\n",
    "                matrix = compute_single_connectivity(epoch_data, band_range)\n",
    "                if matrix is not None:\n",
    "                    subject_matrices[(condition, band_name)] = matrix\n",
    "                else:\n",
    "                    print(f\"⚠️  Failed to compute {band_name} for {condition} in {subject}\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Subject {subject} failed: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return subject_matrices\n",
    "\n",
    "\n",
    "def batch_process_subjects(subjects: List[str]) -> Dict[Tuple[str, str], List[np.ndarray]]:\n",
    "    \"\"\"Process all subjects in parallel and collect matrices.\"\"\"\n",
    "    print(f\"🚀 Processing {len(subjects)} subjects in parallel using {N_JOBS} cores...\")\n",
    "    \n",
    "    all_matrices = defaultdict(list)\n",
    "    \n",
    "    # Process subjects in parallel\n",
    "    with ProcessPoolExecutor(max_workers=N_JOBS) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_subject = {\n",
    "            executor.submit(process_subject_parallel, subject): subject \n",
    "            for subject in subjects\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_subject):\n",
    "            subject = future_to_subject[future]\n",
    "            try:\n",
    "                subject_matrices = future.result()\n",
    "                \n",
    "                # Add matrices to collections\n",
    "                for (condition, band_name), matrix in subject_matrices.items():\n",
    "                    all_matrices[(condition, band_name)].append(matrix)\n",
    "                \n",
    "                completed += 1\n",
    "                success_count = len(subject_matrices)\n",
    "                print(f\"✅ {subject}: {success_count} matrices ({completed}/{len(subjects)})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {subject}: failed with exception ({completed}/{len(subjects)}) — {e}\")\n",
    "                completed += 1\n",
    "    \n",
    "    return dict(all_matrices)\n",
    "\n",
    "\n",
    "def compute_fast_averages(all_matrices: Dict[Tuple[str, str], List[np.ndarray]]) -> Dict[Tuple[str, str], np.ndarray]:\n",
    "    \"\"\"Compute group averages using optimized numpy operations.\"\"\"\n",
    "    print(f\"\\n⚡ Computing group averages...\")\n",
    "    \n",
    "    group_averages = {}\n",
    "    for (condition, band_name), matrix_list in all_matrices.items():\n",
    "        if not matrix_list:\n",
    "            print(f\"⚠️  No data for {condition} {band_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Stack and average in one optimized operation\n",
    "        stacked = np.stack(matrix_list, axis=0, dtype=np.float32)\n",
    "        group_avg = np.mean(stacked, axis=0, dtype=np.float32)\n",
    "        \n",
    "        group_averages[(condition, band_name)] = group_avg\n",
    "        print(f\"   ✅ {condition} {band_name}: {len(matrix_list)} subjects\")\n",
    "    \n",
    "    return group_averages\n",
    "\n",
    "\n",
    "def save_matrices_fast(group_averages: Dict[Tuple[str, str], np.ndarray], roi_names: List[str]) -> None:\n",
    "    \"\"\"Save only the final group average matrices as CSV.\"\"\"\n",
    "    print(f\"\\n💾 Saving {len(group_averages)} group average matrices...\")\n",
    "    \n",
    "    for (condition, band_name), matrix in group_averages.items():\n",
    "        df = pd.DataFrame(matrix, index=roi_names, columns=roi_names)\n",
    "        csv_filename = f\"matrix_{condition}_{band_name}_group_avg.csv\"\n",
    "        csv_filepath = GROUP_OUTPUT_DIR / csv_filename\n",
    "        df.to_csv(csv_filepath, float_format='%.6f')\n",
    "        print(f\"   ✅ {csv_filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with timing.\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting ultra-fast group connectivity analysis with PHASE SEGMENTS...\")\n",
    "    \n",
    "    # Load ROI names once\n",
    "    print(\"📥 Loading ROI names...\")\n",
    "    roi_names = load_roi_names()\n",
    "    \n",
    "    # Find subjects\n",
    "    subjects = find_subjects()\n",
    "    if not subjects:\n",
    "        print(\"❌ No valid subjects found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🧬 Found {len(subjects)} subjects\")\n",
    "\n",
    "    # 🆕 Ensure epochs exist (auto-create if missing)\n",
    "    ensure_epochs_exist(subjects)\n",
    "\n",
    "    # Re-find subjects (in case some were created now)\n",
    "    subjects = find_subjects()\n",
    "    if not subjects:\n",
    "        print(\"❌ Still no valid subjects after epoch creation.\")\n",
    "        return\n",
    "\n",
    "    # Phase 1: Extract all matrices in parallel\n",
    "    all_matrices = batch_process_subjects(subjects)\n",
    "    \n",
    "    if not all_matrices:\n",
    "        print(\"❌ No matrices computed.\")\n",
    "        return\n",
    "    \n",
    "    # Phase 2: Compute averages\n",
    "    group_averages = compute_fast_averages(all_matrices)\n",
    "    \n",
    "    if not group_averages:\n",
    "        print(\"❌ No group averages computed.\")\n",
    "        return\n",
    "    \n",
    "    # Phase 3: Save CSVs\n",
    "    save_matrices_fast(group_averages, roi_names)\n",
    "    \n",
    "    # Summary\n",
    "    elapsed = time.time() - start_time\n",
    "    total_subjects = sum(len(matrices) for matrices in all_matrices.values())\n",
    "    \n",
    "    print(f\"\\n🎉 Analysis Complete!\")\n",
    "    print(f\"   • Time elapsed: {elapsed:.1f} seconds\")\n",
    "    print(f\"   • Subjects found: {len(subjects)}\")\n",
    "    print(f\"   • Total matrices computed: {total_subjects}\")\n",
    "    print(f\"   • Conditions: {CONDITIONS}\")\n",
    "    print(f\"   • Frequency bands: {list(BANDS.keys())}\")\n",
    "    print(f\"   • Output: {GROUP_OUTPUT_DIR}\")\n",
    "    print(f\"   • Speed: {total_subjects/elapsed:.1f} matrices/second\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xtra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
